# -*- coding: utf-8 -*-
"""nnchona.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hrsT952YQKAoabH-WCD23LzWvZN3HGzX

# Simple Neural Network

Simple Red Neuronal (Perceptrón) para la materia Tecnologías Exponenciales - TIC

Inspirado en https://github.com/jonasbostoen/simple-neural-network

### Librerias
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import HTML, display
import tabulate
import networkx as nx
from random import random as rand
import itertools

np.random.seed(1)

"""### Funciones"""

def sigmoid(x):
    return 1 / ( 1 + np.exp(-x) )

# sigmoid = lambda x : 1 / ( 1 + np.exp(-x) )

def sigmoid_derivative(x):
    return x * (1 - x)

# sigmoid_derivative = lambda x : x * (1 - x)

"""Sigmoid Function
$$\operatorname{f}(x)=\frac{1}{1+e^{(-x)}}$$

Sigmoid Derivative Function
$$\operatorname{f'}(x)=x \times (1-x)$$

#### Visualización Función Sigmoide
"""

x = np.linspace(-5,5,100)
y = sigmoid(x)
fig = plt.figure()
ax = fig.add_subplot(1, 1, 1)

# Move left y-axis and bottim x-axis to centre, passing through (0,0)
ax.spines['left'].set_position('center')

# Eliminate upper and right axes
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')

plt.plot(x, y)
plt.locator_params(axis='y', nbins=3)
plt.yticks(fontsize=18)
plt.xticks(fontsize=18)
plt.grid()

"""## Setup

### Values

#### Training Values
"""

training_inputs = np.array([
    [1,1,0,0,1],   #mujer bonita
    [0,0,1,1,0],   #avengers
    [0,0,1,1,0],   #romeo debe morir
    [1,1,1,1,0],   #sr y sra smith
    [1,1,0,0,0],   #orgullo y prejuicio
    [1,0,0,1,1],   #son como niños
    [1,0,1,1,1]    #top secret
])

training_outputs = np.array([
    [1, 0, 1],
    [0, 1, 0],
    [0, 1, 1],
    [1, 1, 1],
    [0, 0, 1],
    [1, 0, 0],
    [1, 1, 0]
])

display(training_inputs)
display(training_outputs)

training_values = np.concatenate((training_inputs.T, training_outputs.T)).T
table = lambda arr : display(HTML(tabulate.tabulate(arr, tablefmt='html')))
table(training_values)

"""#### Synaptic weights"""

n = training_inputs.shape[1]
synaptic_weights = 2 + np.random.random((5, 3)) - 1

synaptic_weights

"""## Training

Para obtener un resultado a partir de los inputs y los synaptic weights se realiza la siguiente función:

$$\sum_{i=0}^{n}{x_iw_i}$$

Y se le aplica la función sigmoide para normalizar al rango 0, 1

Por ejemplo, si nuestros inputs $x_i$ son `[1, 1, 1]` y nuestros pesos sinapticos $w_i$ son `[[11.30926442], [-0.20511667], [-5.4500033]]`

Nos resulta la cuenta
$$[1 \times 11.30926442] + [1 \times (-0.20511667)] + [1 \times (-5.4500033)] = 5.6541444499999995$$

Con la función sigmoide es facil ver como $$\operatorname{sigmoid}(5.65) \approx 1$$
"""

eval_NN = lambda x, w: sigmoid(np.dot(x, w))

# 50000 epochs
for iteration in range(50000):

    # Arranca con los valores de training
    input_layer = training_inputs

    # Genera un output haciendo la suma de los valores de input multiplicados por su peso sinaptico, y aplicandole la función sigmoide para normalizar a -1, 1
    outputs = eval_NN(input_layer, synaptic_weights)

    # Calculo el error
    error = training_outputs - outputs

    # Ajusto los pesos en funcion al error y al resultado obtenido
    adjustments = error * sigmoid_derivative(outputs)
    synaptic_weights += np.dot(input_layer.T, adjustments)

print('Synaptic weights after training')
print(synaptic_weights)

print('Outputs after training: ')
print(outputs)

"""## Eval"""

large = lambda x : display(HTML('<p style="font-size: 50px; color: #66D148">'+str(x)+'</p>'))

import numpy as np

#synaptic_weights = np.random.uniform(size=(5, 3))

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

a = 1 #@param {type:"integer"}
b = 0 #@param {type:"integer"}
c = 0 #@param {type:"integer"}
d = 1 #@param {type:"integer"}
e = 1 #@param {type:"integer"}

input_layer = np.array([a, b, c, d, e])
res = sigmoid(np.dot(input_layer, synaptic_weights))

thresh = 0.5
out = (res >= thresh).astype(int)

display(res)
large(out)

"""### Graph"""

# Define the number of inputs and outputs
num_inputs = 5
num_outputs = 3

# Create a directed graph
G = nx.DiGraph()

# Define the layers for the neural network
layers = [[i for i in range(num_inputs)], [o for o in range(num_inputs, num_inputs + num_outputs)]]

# Generate all possible connections between layers
rels = list(itertools.product(*layers))

# Define node labels for inputs and outputs
input_labels = [f"Input {i}" for i in range(num_inputs)]
output_labels = [f"Output {o - num_inputs}" for o in range(num_inputs, num_inputs + num_outputs)]
labels = {i: input_labels[i] for i in range(num_inputs)}
labels.update({o: output_labels[o - num_inputs] for o in range(num_inputs, num_inputs + num_outputs)})

# Define positions for nodes in the graph
pos = {}
for i, l in enumerate(layers):
    for j, v in enumerate(l):
        pos[v] = (i, len(l) - j - 1)

# Define edge weights (you'll need to replace this with your actual synaptic weights)

#weights_labels = {(ra, rb): "{:0.3f}".format(synaptic_weights[i][j]) for i, (ra, rb) in enumerate(rels) for j in range(num_outputs)}

weights_labels = {(ra, rb): "{:0.3f}".format(synaptic_weights[ra][rb - num_inputs]) for ra, rb in rels}

# Add edges to the graph
G.add_edges_from(rels)

# Define visualization options
options = {
    "font_size": 12,
    "node_size": 3000,
    "node_color": [(1, 1, 1, 1) for _ in range(len(labels))],
    "edgecolors": "black",
    "linewidths": 3,
    "width": 3,
    "labels": labels,
    "connectionstyle": "arc3, rad=0",
}

# Draw the graph
nx.draw_networkx(G, pos, **options)

# Draw edge labels
nx.draw_networkx_edge_labels(
    G, pos,
    edge_labels=weights_labels,
    font_color='black',
    font_size=12,
    font_weight='bold'
)

# Display the graph
plt.axis('off')
ax = plt.gca()
ax.margins(0.20)
plt.show()

"""# **Bonus**

**A. ¿Como modificarían la estructura de la red para que admita *k* capas ocultas con *n* neuronas cada una? Describir como sería el algoritmo de forward propagation en este caso, no es necesario implementar el algoritmo de back propagation.**

La modificación de la estructura de la red comenzaría encontrando los nuevos pesos de cada entrada y el sesgo. Luego, este nuevo algoritmo de forward propagation causaría que se calculen de nuevo todos los valores de las capas sumando el producto de los valores de entrada, los pesos y añadiendo el sesgo. Luego, se les aplica la función de activación (como sucede en los procesos de forward propagation) y la salida de una capa se convierte en la entrada para la siguiente capa.

**B. Investigar la función de activación Softmax y explicar como se puede utilizar para resolver un problema de clasificación con *m* clases mutualmente excluyentes. ¿Que habría que modificar en el código para utilizar esta función de activación?**

![image.png](data:image/png;base64,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)

Es una función de activación en la capa de salida para problemas de clasificación multiclase. Toma un conjunto de valores, logits, que representan puntajes para diferentes clases, y los convierte en probabilidades. Un código que ejemplifica como se puede usar (con las propiedades de un modelo secuencial ya construido) sería:

```
import tensorflow as tf

# modelo secuencial
model = tf.keras.Sequential([
    # las capas ocultas
    tf.keras.layers.Dense(10, activation='softmax')
])

# compila modelo con loss adecuado
model.compile(loss='categorical_crossentropy')

# entrena el modelo con los datos de entrenamiento
model.fit(train_data, train_labels, epochs=20)

# hace las predicciones
predicciones = model.predict(nueva_data)
```


---


Para usar Softmax en el código actual, habría que crear una nueva función dentro del programa. Por ejemplo:


```
import numpy as np

def softmax(vector):
    e = np.exp(vector)
    return e / e.sum()
```
Esta función toma un vector y usa "exp" de la librería numpy para calcular "e" (Euler, aproximadamente 2.718) elevado a "x" (cada número del vector).
"""